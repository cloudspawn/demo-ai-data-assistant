# =============================================================================
# Ollama Configuration (Local LLM - Default)
# =============================================================================
# If running on same machine: http://localhost:11434
# If VM accessing PC GPU: http://192.168.x.x:11434 (replace x.x with your PC IP)
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3.1

# Setup instructions:
# 1. Install Ollama: curl -fsSL https://ollama.com/install.sh | sh
# 2. Pull model: ollama pull llama3.1
# 3. Start server: ollama serve

# =============================================================================
# Alternative: Claude API (Optional - Better Quality)
# =============================================================================
# Uncomment and configure if you want to use Claude API instead of Ollama
# Get free API key at: https://console.anthropic.com (Free tier: $5 credits)
# LLM_PROVIDER=anthropic
# ANTHROPIC_API_KEY=sk-ant-...
# ANTHROPIC_MODEL=claude-sonnet-4-20250514

# =============================================================================
# Database Configuration
# =============================================================================
DUCKDB_PATH=data/sample_warehouse.duckdb

# =============================================================================
# API Server Configuration
# =============================================================================
API_HOST=0.0.0.0
API_PORT=8000